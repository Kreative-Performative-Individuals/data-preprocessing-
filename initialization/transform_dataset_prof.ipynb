{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append('C:\\\\Users\\\\mcapo\\\\data-preprocessing-\\\\data-preprocessing-')\n",
    "\n",
    "from dataprocessing_functions import machine\n",
    "\n",
    "#code to assign anomaly\n",
    "file_path = 'smart_app_data.pkl'\n",
    "df = pd.read_pickle(file_path)\n",
    "\n",
    "asset_ids = df['asset_id'].unique().tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in asset_ids:\n",
    "    key = [key for key, val in machine.items() if a in val]\n",
    "    df.loc[df['asset_id'] == a, 'name'] = key[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['operation']=np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['kpi']=='working_time', 'operation']='working'\n",
    "df.loc[df['kpi']=='working_time', 'kpi']='time'\n",
    "df.loc[df['kpi']=='idle_time', 'operation']='idle'\n",
    "df.loc[df['kpi']=='idle_time', 'kpi']='time'\n",
    "df.loc[df['kpi']=='offline_time', 'operation']='offline'\n",
    "df.loc[df['kpi']=='offline_time', 'kpi']='time'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corrected code to drop rows where 'kpi' is 'cost_working' or 'cost_idle'\n",
    "df.drop(df[df['kpi'] == 'cost_working'].index, inplace=True)\n",
    "df.drop(df[df['kpi'] == 'cost_idle'].index, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['kpi']=='consumption', 'operation']='working'\n",
    "df.loc[df['kpi']=='consumption_idle', 'operation']='offline'\n",
    "df.loc[df['kpi']=='consumption_working', 'operation']='idle'\n",
    "df.loc[df['kpi']=='consumption_idle', 'kpi']='consumption'\n",
    "df.loc[df['kpi']=='consumption_working', 'kpi']='consumption'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['kpi']=='power', 'operation']='independent'\n",
    "df.loc[df['kpi']=='cost', 'operation']='independent'\n",
    "df.loc[df['kpi']=='cycles', 'operation']='working'\n",
    "df.loc[df['kpi']=='good_cycles', 'operation']='working'\n",
    "df.loc[df['kpi']=='bad_cycles', 'operation']='working'\n",
    "df.loc[df['kpi']=='average_cycle_time', 'operation']='working'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('C:\\\\Users\\\\mcapo\\\\data-preprocessing-\\\\data-preprocessing-')\n",
    "from dataprocessing_functions import fields, features, identity, check_f_consistency, kpi, get_batch, update_counter, imputer, get_counter, faulty_aq_tol, update_batch\n",
    "from datetime import datetime \n",
    "from collections import OrderedDict\n",
    "\n",
    "def validate(x):\n",
    "\n",
    "    for f in fields:\n",
    "        x.setdefault(f, np.nan) #if some fields is missing from the expected ones, put a nan\n",
    "    x = dict(OrderedDict((key, x[key]) for key in fields)) # order the fields of the datapoint\n",
    "\n",
    "    # Ensure the reliability of the field time\n",
    "    if pd.isna(x['time']):\n",
    "        x['time'] = datetime.now()\n",
    "\n",
    "    # Check that there is no missing information in the identity of the datapoint, otherwise we store in the database, labelled 'Corrupted'.\n",
    "    if any(pd.isna(x.get(key)) for key in identity):\n",
    "        update_counter(x)\n",
    "        x['status']='Corrupted'\n",
    "        return x\n",
    "    # Check if all the features that the datapoint has are nan or missing.\n",
    "    elif all(pd.isna(x.get(key)) for key in features):\n",
    "        update_counter(x)\n",
    "        x['status']='Corrupted'\n",
    "        return x\n",
    "    \n",
    "    #if the datapoint comes here it means that it didn't miss any information about the identity and at least one feature that is not nan.\n",
    "\n",
    "    x=check_range(x) # the flag is to take trace if the datapoint has naturally nans or nans are the result of validation checks.\n",
    "\n",
    "    #if the datapoint comes here it means that at least one feature value is respecting the range constraint for the specific kpi.\n",
    "    if x:\n",
    "        # Check if the features (min, max, sum, avg) satisfy the basic logic rule min<=avg<=max<=sum\n",
    "        cc=check_f_consistency(x)\n",
    "        if all(not c for c in cc): #meaning that no feature respect the logic rule\n",
    "            update_counter(x)\n",
    "            x['status']='Corrupted'\n",
    "            return x\n",
    "        elif all(c for c in cc): #the datapoint verifies the logic rule.\n",
    "                            #if now there is a nan it could be either the result of the range check or that the datapoint intrinsically has these nans.\n",
    "            any_nan=False\n",
    "            for f in features:\n",
    "                if np.isnan(x[f]):\n",
    "                    any_nan=True\n",
    "                    if all(np.isnan(get_batch(x, f))):\n",
    "                        pass\n",
    "                    else:\n",
    "                        update_counter(x)\n",
    "                        break\n",
    "            if any_nan==False:\n",
    "                                 #it means that the datapoint is consistent and it doesn't have nan values --> it is perfect.\n",
    "                update_counter(x, True) #reset the counter.\n",
    "        else: #it means that some feature are consistent and some not. Put at nan the not consistent ones.\n",
    "            for f, c in zip(features, cc):\n",
    "                if c==False:\n",
    "                    x[f]=np.nan\n",
    "            update_counter(x)\n",
    "        x['status']='A/N'\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "def check_range(x):\n",
    "\n",
    "    #Retrieve the specific range for the kpi that we are dealing with\n",
    "    l_thr=kpi[x['kpi']][0][0]\n",
    "    h_thr=kpi[x['kpi']][0][1]\n",
    "\n",
    "    for k in features:\n",
    "        if x[k]<l_thr:\n",
    "            x[k]=np.nan\n",
    "        if k in ['avg', 'max', 'min', 'var'] and x[k]>h_thr:\n",
    "            x[k]=np.nan\n",
    "\n",
    "    # if after checking the range all features are nan --> corrupted\n",
    "    if all(np.isnan(value) for value in [x.get(key) for key in features]):\n",
    "        update_counter(x)\n",
    "        x['status']='Corrupted'\n",
    "    return x\n",
    "\n",
    "def check_range_ai(x):\n",
    "    flag=True #takes trace of: has the datapoint passed the range check without being changed?\n",
    "    l_thr=kpi[x['kpi']][0][0]\n",
    "    h_thr=kpi[x['kpi']][0][1]\n",
    "\n",
    "    for k in features:\n",
    "        if x[k]<l_thr:\n",
    "            flag=False\n",
    "        if k in ['avg', 'max', 'min', 'var'] and x[k]>h_thr:\n",
    "            flag=False\n",
    "    return flag\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "def predict_missing(batch):\n",
    "    seasonality=7\n",
    "    cleaned_batch= [x for x in batch if not np.isnan(x)]\n",
    "    if not(all(pd.isna(x) for x in batch)) and batch:\n",
    "        if len(cleaned_batch)>2*seasonality:\n",
    "            model = ExponentialSmoothing(cleaned_batch, seasonal='add', trend='add', seasonal_periods=seasonality)\n",
    "            model_fit = model.fit()\n",
    "            prediction = model_fit.forecast(steps=1)[0]\n",
    "        else:\n",
    "            prediction=np.nanmean(batch)\n",
    "        return prediction\n",
    "    else: \n",
    "        return np.nan # Leave the feature as nan since we don't have any information in the batch to make the imputation. If the datapoint has a nan because the feature is not definable for it, it will be leaved as it is from the imputator.\n",
    "\n",
    "# ______________________________________________________________________________________________\n",
    "# This function is the one managing the imputation for all the features of the data point  receives as an input the new data point, extracts the information\n",
    "\n",
    "def imputer(x):\n",
    "    if x:\n",
    "        if isinstance(x, tuple):\n",
    "            x = x[0]\n",
    "            #Because the validated datapoint may exit in the check range with 2 returned values.\n",
    "\n",
    "        # Try imputation with mean or the HWES model.\n",
    "        for f in features:\n",
    "            batch = get_batch(x, f)\n",
    "            if pd.isna(x[f]):\n",
    "                    x[f]=predict_missing(batch)\n",
    "\n",
    "        # Check again the consistency of features and the range.\n",
    "        if check_f_consistency(x) and check_range_ai(x):\n",
    "            pass\n",
    "        else:  # It means that the imputed data point has not passed the check on the features and on their expected range.\n",
    "            # In this case we use the LVCF as a method of imputation since it ensures the respect of these conditiono (the last point in the batch has been preiovusly checked)\n",
    "            for f in features:\n",
    "                batch = get_batch(x, f)\n",
    "                x[f]=batch[-1]\n",
    "        \n",
    "        # In the end update batches with the new data point\n",
    "        for f in features:\n",
    "            update_batch(x, f, x[f])\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['status']=np.nan\n",
    "cleaned_df=df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/22368\n",
      "100/22368\n",
      "200/22368\n",
      "300/22368\n",
      "400/22368\n",
      "500/22368\n",
      "600/22368\n",
      "700/22368\n",
      "800/22368\n",
      "900/22368\n",
      "1000/22368\n",
      "1100/22368\n",
      "1200/22368\n",
      "1300/22368\n",
      "1400/22368\n",
      "1500/22368\n",
      "1600/22368\n",
      "1700/22368\n",
      "1800/22368\n",
      "1900/22368\n",
      "2000/22368\n",
      "2100/22368\n",
      "2200/22368\n",
      "2300/22368\n",
      "2400/22368\n",
      "2500/22368\n",
      "2600/22368\n",
      "2700/22368\n",
      "2800/22368\n",
      "2900/22368\n",
      "3000/22368\n",
      "3100/22368\n",
      "3200/22368\n",
      "3300/22368\n",
      "3400/22368\n",
      "3500/22368\n",
      "3600/22368\n",
      "3700/22368\n",
      "3800/22368\n",
      "3900/22368\n",
      "4000/22368\n",
      "4100/22368\n",
      "4200/22368\n",
      "4300/22368\n",
      "4400/22368\n",
      "4500/22368\n",
      "4600/22368\n",
      "4700/22368\n",
      "4800/22368\n",
      "4900/22368\n",
      "5000/22368\n",
      "5100/22368\n",
      "5200/22368\n",
      "5300/22368\n",
      "5400/22368\n",
      "5500/22368\n",
      "5600/22368\n",
      "5700/22368\n",
      "5800/22368\n",
      "5900/22368\n",
      "6000/22368\n",
      "6100/22368\n",
      "6200/22368\n",
      "6300/22368\n",
      "6400/22368\n",
      "6500/22368\n",
      "6600/22368\n",
      "6700/22368\n",
      "6800/22368\n",
      "6900/22368\n",
      "7000/22368\n",
      "7100/22368\n",
      "7200/22368\n",
      "7300/22368\n",
      "7400/22368\n",
      "7500/22368\n",
      "7600/22368\n",
      "7700/22368\n",
      "7800/22368\n",
      "7900/22368\n",
      "8000/22368\n",
      "8100/22368\n",
      "8200/22368\n",
      "8300/22368\n",
      "8400/22368\n",
      "8500/22368\n",
      "8600/22368\n",
      "8700/22368\n",
      "8800/22368\n",
      "8900/22368\n",
      "9000/22368\n",
      "9100/22368\n",
      "9200/22368\n",
      "9300/22368\n",
      "9400/22368\n",
      "9500/22368\n",
      "9600/22368\n",
      "9700/22368\n",
      "9800/22368\n",
      "9900/22368\n",
      "10000/22368\n",
      "10100/22368\n",
      "10200/22368\n",
      "10300/22368\n",
      "10400/22368\n",
      "10500/22368\n",
      "10600/22368\n",
      "10700/22368\n",
      "10800/22368\n",
      "10900/22368\n",
      "11000/22368\n",
      "11100/22368\n",
      "11200/22368\n",
      "11300/22368\n",
      "11400/22368\n",
      "11500/22368\n",
      "11600/22368\n",
      "11700/22368\n",
      "11800/22368\n",
      "11900/22368\n",
      "12000/22368\n",
      "12100/22368\n",
      "12200/22368\n",
      "12300/22368\n",
      "12400/22368\n",
      "12500/22368\n",
      "12600/22368\n",
      "12700/22368\n",
      "12800/22368\n",
      "12900/22368\n",
      "13000/22368\n",
      "13100/22368\n",
      "13200/22368\n",
      "13300/22368\n",
      "13400/22368\n",
      "13500/22368\n",
      "13600/22368\n",
      "13700/22368\n",
      "13800/22368\n",
      "13900/22368\n",
      "14000/22368\n",
      "14100/22368\n",
      "14200/22368\n",
      "14300/22368\n",
      "14400/22368\n",
      "14500/22368\n",
      "14600/22368\n",
      "14700/22368\n",
      "14800/22368\n",
      "14900/22368\n",
      "15000/22368\n",
      "15100/22368\n",
      "15200/22368\n",
      "15300/22368\n",
      "15400/22368\n",
      "15500/22368\n",
      "15600/22368\n",
      "15700/22368\n",
      "15800/22368\n",
      "15900/22368\n",
      "16000/22368\n",
      "16100/22368\n",
      "16200/22368\n",
      "16300/22368\n",
      "16400/22368\n",
      "16500/22368\n",
      "16600/22368\n",
      "16700/22368\n",
      "16800/22368\n",
      "16900/22368\n",
      "17000/22368\n",
      "17100/22368\n",
      "17200/22368\n",
      "17300/22368\n",
      "17400/22368\n",
      "17500/22368\n",
      "17600/22368\n",
      "17700/22368\n",
      "17800/22368\n",
      "17900/22368\n",
      "18000/22368\n",
      "18100/22368\n",
      "18200/22368\n",
      "18300/22368\n",
      "18400/22368\n",
      "18500/22368\n",
      "18600/22368\n",
      "18700/22368\n",
      "18800/22368\n",
      "18900/22368\n",
      "19000/22368\n",
      "19100/22368\n",
      "19200/22368\n",
      "19300/22368\n",
      "19400/22368\n",
      "19500/22368\n",
      "19600/22368\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "length=df.shape[0]//4\n",
    "for i in range(length):\n",
    "    if i%100==0:\n",
    "        print(f'{i}/{length}')\n",
    "    datapoint=df.iloc[i].to_dict()\n",
    "    old_counter=get_counter(datapoint)\n",
    "    #print(f'original datapoint: {datapoint}')\n",
    "    datapoint=validate(datapoint)\n",
    "    new_counter=get_counter(datapoint)\n",
    "    if new_counter==old_counter+1 and new_counter>=faulty_aq_tol:\n",
    "        id = {key: datapoint[key] for key in identity if key in datapoint}\n",
    "        f\"It has been {new_counter} days (from {datapoint['time']} that {id['name']} - {id['asset_id']} returns NaN values in {id['kpi']} - {id['operation']}. Possible malfunctioning either in the acquisition system or in the machine!\"\n",
    "    if datapoint['status']!='Corrupted':\n",
    "        cleaned_datapoint=imputer(datapoint)\n",
    "    cleaned_df.iloc[i]=cleaned_datapoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'i' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mi\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'i' is not defined"
     ]
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data=[cleaned_df.to_dict(), i]\n",
    "with open('C:\\\\Users\\\\mcapo\\\\data-preprocessing-\\\\data-preprocessing-\\\\initialization\\\\transformation_interrupted.json', \"w\") as json_file:\n",
    "    json.dump(data, json_file, indent=1) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SmartApp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
