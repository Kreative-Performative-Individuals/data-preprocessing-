{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EIKOcZVO7HF_"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import feature_engineering_functions as fef\n",
    "import machine_learning_functions as mlf\n",
    "\n",
    "# Load the dataset\n",
    "# Load the .pkl file\n",
    "file_path = 'smart_app_data.pkl'\n",
    "historical_data = pd.read_pickle(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RAU2Vdif7jqJ",
    "outputId": "9eb6bcc9-3cd1-41fc-f3a0-1f6fd76bb6e5"
   },
   "outputs": [],
   "source": [
    "kpis = historical_data['kpi'].unique()\n",
    "print(\"------------ kpis are ----------------\")\n",
    "print(kpis)\n",
    "\n",
    "machines = historical_data['name'].unique()\n",
    "print(\"------------ machines are ----------------\")\n",
    "print(machines)\n",
    "\n",
    "'''operations = historical_data['operation'].unique()\n",
    "print(\"------------ operations are ----------------\")\n",
    "print(operations)'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SJGRL-9n7sS8",
    "outputId": "d56a7a01-a58e-414b-d5cb-e80cf887dae1"
   },
   "outputs": [],
   "source": [
    "def extract_features(kpi_name, machine_name, operation_name, data):\n",
    "\n",
    "  # function that is able to extract time series filtering for:\n",
    "  # kpi_vame = name of the kpi\n",
    "  # kpi_value = which should be chose in the list ['sum', 'avg','min', 'max']\n",
    "  # machine_name = name of the machine\n",
    "  # data = name of the dataframe from which data are provided\n",
    "\n",
    "  filtered_data = data[(data[\"name\"] == machine_name) &\n",
    "                    (data[\"kpi\"] == kpi_name)]\n",
    "  #later replace by\n",
    "  #filtered_data = data[(data[\"name\"] == machine_name) &\n",
    "  # (data[\"kpi\"] == kpi_name)] & (data[\"operation\"] == operation_name)]\n",
    "  filtered_data['time'] = pd.to_datetime(filtered_data['time'])\n",
    "  filtered_data = filtered_data.sort_values(by='time')\n",
    "\n",
    "  return filtered_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JJ-kdwoN_XxX"
   },
   "outputs": [],
   "source": [
    "# The input dataframe corresponds to a filtrate version of the dataset for a given machine, kpi and operation,\n",
    "# so it contains 9 columns and the amount of entries correspondent to the selected time range.\n",
    "\n",
    "def feature_engineering_pipeline(dataframe, kwargs):\n",
    "    features = ['sum', 'avg','min', 'max', 'var']\n",
    "    for feature_name in features:\n",
    "        # Check if the column exists in the DataFrame\n",
    "        if feature_name in dataframe.columns:\n",
    "            print(\"-------------------- Results for \" + str(feature_name))\n",
    "            feature = dataframe[feature_name]\n",
    "            if feature.empty or feature.isna().all() or feature.isnull().all():\n",
    "                print(\"Feature is empty (no data).\")\n",
    "            else:\n",
    "                ## Check stationarity \n",
    "                # (output is False if not stationary, True if it is, None if test couldn't be applied)\n",
    "                is_stationary = fef.adf_test(feature.dropna()) \n",
    "                print('Output is stationary? ' + str(is_stationary))\n",
    "            \n",
    "                ## Check seasonality\n",
    "                # (output: period of the seasonality None if no seasonalaty was detected.\n",
    "                seasonality_period = fef.detect_seasonality_acf(feature)\n",
    "                print('Seasonality period is? ' + str(seasonality_period))\n",
    "            \n",
    "                #further check in the case the seasonality pattern is complex and cannot be detected\n",
    "                if seasonality_period == None:\n",
    "                    # (output: period of the seasonality None if no seasonalaty was detected.\n",
    "                    seasonality_period = fef.detect_seasonality_fft(feature)\n",
    "                    print('Recomputed seasonality period is? ' + str(seasonality_period))\n",
    "            \n",
    "                # (output: the decomposed time series in a list, of form [trend, seasonal, residual],\n",
    "                # None if it isn't sufficient data or if some error occurs.\n",
    "                decompositions = fef.seasonal_additive_decomposition(feature, seasonality_period) \n",
    "\n",
    "                #Make data stationary / Detrend / Deseasonalize (if needed)\n",
    "            \n",
    "                make_stationary = kwargs.get('make_stationary', False)  # Set default to False if not provided\n",
    "                detrend = kwargs.get('detrend', False) # Set default to False if not provided\n",
    "                deseasonalize = kwargs.get('deseasonalize', False) # Set default to False if not provided\n",
    "                get_residuals = kwargs.get('get_residuals', False) # Set default to False if not provided\n",
    "                scaler = kwargs.get('scaler', False)  # Set default to False if not provided\n",
    "                \n",
    "                if make_stationary and (not is_stationary):\n",
    "                    if decompositions != None:\n",
    "                        feature = fef.make_stationary_decomp(feature, decompositions)\n",
    "                        is_stationary = fef.adf_test(feature.dropna())\n",
    "                        print('Is stationary after trying to make it stationary? ' + str(is_stationary))\n",
    "                        if not is_stationary:\n",
    "                            feature = fef.make_stationary_diff(feature, seasonality_period=[7]) #default weekly\n",
    "                            is_stationary = fef.adf_test(feature.dropna())\n",
    "                            print('Is stationary after re-trying to make it stationary? ' + str(is_stationary))\n",
    "                    else:\n",
    "                        feature = fef.make_stationary_diff(feature, seasonality_period=[7]) #default weekly\n",
    "                        is_stationary = fef.adf_test(feature.dropna())\n",
    "                        print('Is stationary after trying to make it stationary? ' + str(is_stationary))\n",
    "            \n",
    "                if detrend:\n",
    "                    if decompositions != None:\n",
    "                        feature = fef.rest_trend(feature, decompositions)\n",
    "                    else:\n",
    "                        feature = fef.make_stationary_diff(feature)\n",
    "                \n",
    "                if deseasonalize:\n",
    "                    if decompositions != None:\n",
    "                        feature = fef.rest_seasonality(feature, decompositions)\n",
    "                    else:\n",
    "                        feature = fef.make_stationary_diff(feature, seasonality_period=[7]) #default weekly\n",
    "            \n",
    "                if get_residuals:\n",
    "                    if decompositions != None:\n",
    "                        feature = fef.get_residuals(feature, decompositions)\n",
    "                    else:\n",
    "                        feature = fef.make_stationary_diff(feature)\n",
    "                        feature = fef.make_stationary_diff(feature, seasonality_period=[7]) #default weekly\n",
    "                \n",
    "                if scaler:\n",
    "                    # Apply standardization (z-score scaling)\n",
    "                    feature = (feature - np.mean(feature)) / np.std(feature)\n",
    "            \n",
    "            dataframe[feature_name] = feature\n",
    "\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPLEMENTATION: How should this work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, the dictionaries that tell us how to preprocess data for each pourpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML_algorithms_config = {\n",
    "    'forecasting_ffnn': {\n",
    "        'make_stationary': True,  # Default: False\n",
    "        'detrend': True,          # Default: False\n",
    "        'deseasonalize': True,    # Default: False\n",
    "        'get_residuals': True,    # Default: False\n",
    "        'scaler': True             # Default: True\n",
    "    },\n",
    "    'anomaly_detection': {\n",
    "        'make_stationary': False, # Default: False\n",
    "        'detrend': False,         # Default: False\n",
    "        'deseasonalize': False,   # Default: False\n",
    "        'get_residuals': False,    # Default: False\n",
    "        'scaler': False           # Default: True\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, for each machine, KPI and operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#examples of usage\n",
    "kpi = 'good_cycles'\n",
    "machine = 'Large Capacity Cutting Machine 1'\n",
    "operation = \"working\"\n",
    "\n",
    "for machine in machines:\n",
    "    for kpi in kpis:\n",
    "        #for operation in operations:\n",
    "\n",
    "            #extract feature of interest (this should arrive from historical data)\n",
    "            #this function will be in feature_engineering functions\n",
    "\n",
    "            feature_extracted = extract_features(kpi, machine, operation, historical_data)\n",
    "            feature_extracted.reset_index(drop=True, inplace=True)\n",
    "\n",
    "            #this should be run at the beginning of the given operation (forecasting or anomaly detection)\n",
    "            #let's suppose we want to perform anomaly detection\n",
    "            #this dictionary will be in information\n",
    "\n",
    "            processing_config = ML_algorithms_config['anomaly_detection']\n",
    "\n",
    "            #check for presence of drift at last timepoint\n",
    "            #this algorithm will be in machine_learning_functions\n",
    "            drift_presence = mlf.ADWIN_drift(feature_extracted)\n",
    "            \n",
    "            #transform data to become a suitable input to the ML algorithm\n",
    "            transformed_time_series = feature_engineering_pipeline(feature_extracted, processing_config)\n",
    "\n",
    "            # ouput is in the same format as feature_extracted, so is the dataset filtered for machine, kpi\n",
    "            # and operation (to decomment)\n",
    "            if drift_presence:\n",
    "                  #retrain\n",
    "                  pass\n",
    "            \n",
    "            #model usage\n",
    "\n",
    "print('hi')\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
