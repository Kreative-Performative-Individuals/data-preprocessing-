{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt \n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "\n",
    "# from our documents\n",
    "import preprocessing_functions as pf\n",
    "\n",
    "# from Scikit Learn library\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV, ParameterGrid\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, accuracy_score, make_scorer, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, RobustScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SequentialFeatureSelector as SFS\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.tree import plot_tree \n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "import random\n",
    "\n",
    "# from Imb Learn\n",
    "from imblearn.over_sampling import SMOTENC, SMOTE\n",
    "\n",
    "#seed for random processes\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following pipeline aims to automatize the preprocessing of the stream data from time series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will assume that we already have part of the dataset, that will be used as historical data to fill missing values or study seasonalities and stationarities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will particular show what to do whenever new data arrive to the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'smart_app_data.pkl'\n",
    "df = pd.read_pickle(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: (213, 8)\n",
      "Test set size: (92, 8)\n"
     ]
    }
   ],
   "source": [
    "# Split the data into historical and future data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide data by machine and kpis (time series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary about specific preprocessing information for kpis\n",
    "# The idea is to implement it somewhere else and importing it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline will receive as an input the new incoming data for a specific machine and kpi. Also, to perform the preprocessing it need also to receive the batch of a fixed amount of past data and the information about how specifically handle that kpi for that machine (given by kwargs). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_pipeline(batch, new_input, kwargs):\n",
    "\n",
    "    ####### Preprocessing of the data\n",
    "\n",
    "    ### DATA CLEANING\n",
    "\n",
    "    ## Data type standardization\n",
    "\n",
    "    ## Check for inconsistencies\n",
    "    \n",
    "    ## Fill missing values\n",
    "\n",
    "    ## Resampling and smoothing (if needed)\n",
    "\n",
    "    ### FEATURE ENGINEERING\n",
    "    dataframe = batch.append(new_input) #probably it doesn't work in this way, but the idea is to integrate them\n",
    "\n",
    "    ## Check stationarity\n",
    "    is_stationary = pf.adf_test(dataframe.dropna()) #False if not stationary, True if it is, None if test couldn't be applied\n",
    "    \n",
    "    ## Check seasonality \n",
    "    period_of_observation = 'should be extracted from kwargs'\n",
    "    trend, seasonalilty, residual = pf.seasonal_additive_decomposition(dataframe, period_of_observation) \n",
    "    ## Make data stationary / Detrend / Deseason (if needed)\n",
    "\n",
    "    ##Normalization (if needed)\n",
    "    ##One-Hot-Encoding (if needed)\n",
    "\n",
    "    #this are usage examples\n",
    "    categorical_columns = dataframe.select_dtypes(include=['object']).columns\n",
    "    #whether to use encoder or scaler is defined in kwargs\n",
    "\n",
    "    if not encoder: #only scaler\n",
    "        transformers = [('num', RobustScaler(), ~dataframe.columns.isin(categorical_columns))]\n",
    "    elif not scaler: #only encoder\n",
    "        transformers = [('cat', OneHotEncoder(handle_unknown='ignore'), categorical_columns)]\n",
    "    elif not encoder and not scaler:\n",
    "        transformers = []\n",
    "    else: # both scaler and encoder\n",
    "        transformers=[\n",
    "            ('num', RobustScaler(), ~dataframe.columns.isin(categorical_columns)),\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_columns),\n",
    "        ]\n",
    "\n",
    "    preprocessor = ColumnTransformer(transformers=transformers)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ML_pipeline(time_series, kwargs):\n",
    "    \n",
    "    ## Definition of metrics for goodness of model \n",
    "\n",
    "    ## Drift detection algorithms\n",
    "\n",
    "    ### ML ALGORTIHMS (this should be divided on training phase and prediction phase)\n",
    "\n",
    "    ## Check Outliers \n",
    "\n",
    "    ## Feature selection\n",
    "\n",
    "    ## Parameters setting\n",
    "\n",
    "    ## Data forecasting (if selected)\n",
    "\n",
    "    ## Anomalies detection (if selected)\n",
    "\n",
    "    ## Models comparison (if needed)\n",
    "        \n",
    "    return result_of_ML #forecasting prediction or detected anomaly\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
