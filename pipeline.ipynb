{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_cleaning_functions import cleaning_pipeline\n",
    "from information import create_info_object, machine, kpi, features, identity, ML_algorithms_config\n",
    "from machine_learning_functions import AnomalyDetector\n",
    "from machine_learning_functions import ADWIN_drift\n",
    "from machine_learning_functions import tdnn_forecasting_training\n",
    "from machine_learning_functions import tdnn_forecasting_prediction\n",
    "from feature_engineering_functions import feature_engineering_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datapoint():\n",
    "    # In some manner receives data point as a dictionary of form\n",
    "    ''' datapoint={timestamp: datetime, \n",
    "        isset_id: string,\n",
    "        name: string,\n",
    "        kpi: string,\n",
    "        operation: string,\n",
    "        sum: float\n",
    "        avg: float,\n",
    "        min: float,\n",
    "        max: float,\n",
    "        var: float}'''\n",
    "    #for example\n",
    "\n",
    "    datapoint = {\n",
    "    'timestamp': 'timepoint',\n",
    "    'isset_id': 'ast-yhccl1zjue2t',\n",
    "    'name': 'metal_cutting',\n",
    "    'kpi': 'time',\n",
    "    'operation': 'working',\n",
    "    'sum': float, \n",
    "    'avg': float,\n",
    "    'min': float,\n",
    "    'max': float,\n",
    "    'var': float}\n",
    "    \n",
    "    return datapoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_historical_data(machine_name, asset_id, kpi, operation, timestap_start, timestamp_end):\n",
    "    # In some manner receives data frame filtered from the database in format dataframe\n",
    "    #Maybe we can define that if we give timestap_start = None, timestamp_end = None,\n",
    "    #they have to return us x values in the past starting from the last stored point\n",
    "\n",
    "    return filtered_dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_alert(anomaly_identity):\n",
    "    \n",
    "    # In some manner calls the alert function and sends the identity\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_datapoint(new_datapoint):\n",
    "    \n",
    "    # In some manner gives the new_datapoint dictionary to the database, so they can store it\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_request(machine_name, asset_id, kpi, operation, timestap_start, timestamp_end, transformation, forecasting):\n",
    "    \n",
    "\n",
    "    transformation_config = {\n",
    "        'make_stationary': False,  # Default: False\n",
    "        'detrend': False,          # Default: False\n",
    "        'deseasonalize': False,    # Default: False\n",
    "        'get_residuals': False,    # Default: False\n",
    "        'scaler': False             # Default: False\n",
    "    }\n",
    "\n",
    "    if forecasting:\n",
    "        transformation_config['make_stationary'] = True\n",
    "        transformation_config['scaler'] = True\n",
    "        \n",
    "        historical_data = get_historical_data(machine_name, asset_id, kpi, operation, None, None) ## CONNECTION WITH API\n",
    "\n",
    "        transformed_data = feature_engineering_pipeline(historical_data, transformation_config)\n",
    "\n",
    "        forecasting_model_info = im.get_model_forecast(historical_data[-1])\n",
    "\n",
    "        forecasting_model = forecasting_model_info[0]\n",
    "        forecasting_params = forecasting_model_info[1]\n",
    "        forecasting_stats = forecasting_model_info[2]\n",
    "        predictions = tdnn_forecasting_prediction(forecasting_model, forecasting_params['tau'], transformed_data, timestap_start, timestamp_end, forecasting_stats)\n",
    "\n",
    "        return predictions\n",
    "    \n",
    "    else:\n",
    "\n",
    "        historical_data = get_historical_data(machine_name, asset_id, kpi, operation, timestap_start, timestamp_end) ## CONNECTION WITH API\n",
    "\n",
    "        transformed_data = feature_engineering_pipeline(historical_data, transformation_config)\n",
    "\n",
    "        return transformed_data\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing the info manager\n",
    "im = create_info_object()\n",
    "\n",
    "#initializing the anomaly detector\n",
    "ad=AnomalyDetector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "while True: #loops continuosly\n",
    "\n",
    "    #first we call get_datapoint and we wait for a new input to arrive\n",
    "    new_datapoint = get_datapoint() ## CONNECTION WITH API\n",
    "\n",
    "    #once the new data point is aquired we clean it\n",
    "    cleaned_datapoint = cleaning_pipeline(new_datapoint, im)\n",
    "\n",
    "    #we now check if some drift has been detected\n",
    "    drift_flag=ADWIN_drift(cleaned_datapoint, im)\n",
    "\n",
    "    # we need to extract the following from cleaned_datapoint\n",
    "    # machine_name, asset_id, kpi, operation\n",
    "    \n",
    "    #we call the database to extract historical data\n",
    "    historical_data = get_historical_data(machine_name, asset_id, kpi, operation, None, None) ## CONNECTION WITH API\n",
    "    historical_data = historical_data.drop(columns=['anomaly'])\n",
    "    #['anomaly']='Normal'\n",
    "    historical_data.loc[len(get_historical_data)] = cleaned_datapoint\n",
    "\n",
    "    if drift_flag==True:\n",
    "\n",
    "        #retrain anomaly detection model\n",
    "        model=ad.train(historical_data[:]) #Here we should put the get_historical_data()\n",
    "        im.update_model_ad(cleaned_datapoint, model)\n",
    "\n",
    "        #retrain forecasting algorithm model\n",
    "        model_info = tdnn_forecasting_training(historical_data[:])  #contains [best_model_TDNN, best_params, stats] #Here we should put the get_historical_data()\n",
    "        im.update_model_forecast(cleaned_datapoint, model_info)\n",
    "        \n",
    "    #Anomalies detection branch \n",
    "    # get de model    \n",
    "    ad_model=im.get_model_ad(cleaned_datapoint)\n",
    "    #predict class\n",
    "    cleaned_datapoint['anomaly']=ad.predict(cleaned_datapoint , ad_model)\n",
    "\n",
    "    if cleaned_datapoint['anomaly']==\"Anomaly\":\n",
    "        anomaly_identity = {key: cleaned_datapoint[key] for key in identity if key in cleaned_datapoint}\n",
    "        \n",
    "        _ = send_alert(anomaly_identity)\n",
    "    \n",
    "    # MAKE cleaned_datapoint a dictionary \n",
    "    ready_to_store_datapoint = {\n",
    "    'timestamp': 'timepoint',\n",
    "    'isset_id': 'ast-yhccl1zjue2t',\n",
    "    'name': 'metal_cutting',\n",
    "    'kpi': 'time',\n",
    "    'operation': 'working',\n",
    "    'anomaly': str,\n",
    "    'sum': float, \n",
    "    'avg': float,\n",
    "    'min': float,\n",
    "    'max': float,\n",
    "    'var': float}\n",
    "    \n",
    "    _ = store_datapoint(ready_to_store_datapoint) ## CONNECTION WITH API\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Feature forecasting branch\n",
    "#Activation whenever there is a call\n",
    "#num_predictions comes from the query\n",
    "num_predictions = 7 #default\n",
    "forecasting_model_info=im.get_model_forecast(current_dp)\n",
    "transformed_dp = fef.feature_engineering_pipeline(current_dp, infor.ML_algorithms_config)\n",
    "forecasting_model = forecasting_model[0]\n",
    "forecasting_params = forecasting_model[1]\n",
    "forecasting_stats = forecasting_model[2]\n",
    "predictions = tdnn_forecasting_prediction(forecasting_model, forecasting_params['tau'], transformed_dp , num_predictions, forecasting_stats)\n",
    "#Send prediction should be implemented\n",
    "\n",
    "#The feature engineering will be added in a future development."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
