{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt \n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "\n",
    "# from our documents\n",
    "import preprocessing_functions as pf\n",
    "\n",
    "# from Scikit Learn library\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import random\n",
    "\n",
    "#seed for random processes\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following pipeline aims to automatize the preprocessing of the stream data from time series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will assume that we already have part of the dataset, that will be used as historical data to fill missing values or study seasonalities and stationarities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will particular show what to do whenever new data arrive to the system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'smart_app_data.pkl'\n",
    "df = pd.read_pickle(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ensure 'time' column is in datetime format\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "# Sort the dataframe by 'time'\n",
    "df = df.sort_values(by='time')\n",
    "\n",
    "# Define the cutoff date for historical and incoming data\n",
    "cutoff_date = pd.Timestamp('2024-10-01')\n",
    "\n",
    "# Split data into historical (before October 2024) and incoming (after October 2024)\n",
    "# Define the conditions correctly using & and | operators and parentheses for grouping\n",
    "historical_data = df[(df['time'].dt.year < 2024) | ((df['time'].dt.year == 2024) & (df['time'].dt.month < 10))]\n",
    "incoming_data = df[(df['time'].dt.year > 2024) | ((df['time'].dt.year == 2024) & (df['time'].dt.month >= 10))]\n",
    "\n",
    "# Check the first few rows of each dataset to confirm\n",
    "print(\"Historical Data (before October 2024):\")\n",
    "print(historical_data.head())\n",
    "\n",
    "print(\"\\nIncoming Data (after October 2024):\")\n",
    "print(incoming_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of rows in historical data: 47936\n",
      "Number of rows in incoming data: 4256\n",
      "First row in historical data: 2024-03-01 00:00:00+00:00\n",
      "Last row in historical data: 2024-09-30 00:00:00+00:00\n",
      "\n",
      "First row in incoming data: 2024-10-01 00:00:00+00:00\n",
      "Last row in incoming data: 2024-10-19 00:00:00+00:00\n"
     ]
    }
   ],
   "source": [
    "# Check the lengths to confirm the split\n",
    "print(f\"\\nNumber of rows in historical data: {len(historical_data)}\")\n",
    "print(f\"Number of rows in incoming data: {len(incoming_data)}\")\n",
    "\n",
    "# Check first and last rows of historical and incoming data\n",
    "print(f\"First row in historical data: {historical_data.iloc[0]['time']}\")\n",
    "print(f\"Last row in historical data: {historical_data.iloc[-1]['time']}\")\n",
    "\n",
    "print(f\"\\nFirst row in incoming data: {incoming_data.iloc[0]['time']}\")\n",
    "print(f\"Last row in incoming data: {incoming_data.iloc[-1]['time']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All machines are: \n",
      "['Large Capacity Cutting Machine 1', 'Assembly Machine 2', 'Laser Cutter', 'Riveting Machine', 'Assembly Machine 3', 'Medium Capacity Cutting Machine 1', 'Medium Capacity Cutting Machine 2', 'Low Capacity Cutting Machine 1', 'Testing Machine 3', 'Medium Capacity Cutting Machine 3', 'Testing Machine 1', 'Testing Machine 2', 'Laser Welding Machine 1', 'Laser Welding Machine 2', 'Large Capacity Cutting Machine 2', 'Assembly Machine 1']\n",
      "All KPIs are: \n",
      "['working_time', 'consumption_idle', 'consumption_working', 'power', 'consumption', 'offline_time', 'idle_time', 'cost', 'cost_working', 'cost_idle', 'cycles', 'good_cycles', 'bad_cycles', 'average_cycle_time']\n"
     ]
    }
   ],
   "source": [
    "# Divide data by machine and kpis (time series)\n",
    "# Retrieve unique machines and KPIs from the DataFrame\n",
    "machines = df['name'].unique().tolist()\n",
    "\n",
    "print(\"All machines are: \")\n",
    "print(machines)\n",
    "\n",
    "kpis = df['kpi'].unique().tolist()\n",
    "\n",
    "print(\"All KPIs are: \")\n",
    "print(kpis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary about specific preprocessing information for kpis\n",
    "# So the kwargs that will be used in the pipelines are stored there\n",
    "# The idea is to implement it somewhere else and importing it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline will receive as an input the new incoming data for a specific machine and kpi. Also, to perform the preprocessing it need also to receive the batch of a fixed amount of past data and the information about how specifically handle that kpi for that machine (given by kwargs). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_pipeline(batch, new_input, kwargs):\n",
    "\n",
    "    ####### Preprocessing of the data\n",
    "\n",
    "    # Resampling (if needed)\n",
    "    if kwargs.get('resample', False):\n",
    "        new_input= pf.resample_data(batch, new_input, kwargs) #not implemented yet\n",
    "\n",
    "    # Smoothing (if needed, based on kwargs)\n",
    "    if kwargs.get('smooth', False):\n",
    "        new_input = pf.smooth_data(batch, new_input, kwargs) #not implemented yet\n",
    "\n",
    "    ### DATA CLEANING\n",
    "\n",
    "    ## Data type standardization\n",
    "    \n",
    "    # function present preprocessing_functions\n",
    "\n",
    "    ## Check for inconsistencies\n",
    "    \n",
    "    # function present preprocessing_functions\n",
    "\n",
    "    ## Fill missing values\n",
    "\n",
    "    # function present preprocessing_functions\n",
    "\n",
    "    ### FEATURE ENGINEERING\n",
    "    new_batch = pd.concat([batch, new_input]).sort_values(by='timestamp') \n",
    "    # Remove the oldest row (the first row after sorting)\n",
    "    new_batch = new_batch.iloc[1:]\n",
    "\n",
    "    return new_batch, new_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering_pipeline(dataframe, kwargs):\n",
    "    ## Check stationarity\n",
    "    is_stationary = pf.adf_test(dataframe.dropna()) #False if not stationary, True if it is, None if test couldn't be applied\n",
    "    \n",
    "    ## Check seasonality \n",
    "    period_of_observation = kwargs.get('period_of_observation', None)\n",
    "    if period_of_observation is None:\n",
    "        raise ValueError(\"Period of observation must be provided in kwargs.\")\n",
    "    \n",
    "    trend, seasonalilty, residual = pf.seasonal_additive_decomposition(dataframe, period_of_observation) \n",
    "\n",
    "    #Make data stationary / Detrend / Deseasonalize (if needed)\n",
    "    \n",
    "    make_stationary = kwargs.get('need_stationarity', False)  # Set default to False if not provided\n",
    "    detrend = kwargs.get('detrend', False) \n",
    "    deseasonalize = kwargs.get('deseasonalize', False) \n",
    "    \n",
    "    if make_stationary and not is_stationary:\n",
    "        dataframe = pf.make_stationary(dataframe, kwargs) #not implemented yet\n",
    "\n",
    "    if detrend and deseasonalize:\n",
    "        dataframe = pf.rest_trend(dataframe, kwargs) #not implemented yet\n",
    "        dataframe = pf.rest_seasonality(dataframe, kwargs) #not implemented yet\n",
    "    elif detrend:\n",
    "        dataframe = pf.rest_trend(dataframe, kwargs) #not implemented yet\n",
    "    elif deseasonalize:\n",
    "        dataframe = pf.rest_seasonality(dataframe, kwargs) #not implemented yet\n",
    "\n",
    "    # Normalize data and apply encoding (if needed)\n",
    "    categorical_columns = dataframe.select_dtypes(include=['object']).columns\n",
    "\n",
    "    # Determine if encoder or scaler should be applied based on kwargs\n",
    "    encoder = kwargs.get('encoder', False)  # Set default to False if not provided\n",
    "    scaler = kwargs.get('scaler', True)  # Set default to True if not provided\n",
    "    \n",
    "    # Apply transformations based on encoder and scaler flags\n",
    "    if not encoder:  # Only scaler\n",
    "        transformers = [('num', RobustScaler(), ~dataframe.columns.isin(categorical_columns))]\n",
    "    elif not scaler:  # Only encoder\n",
    "        transformers = [('cat', OneHotEncoder(handle_unknown='ignore'), categorical_columns)]\n",
    "    else:  # Both encoder and scaler\n",
    "        transformers = [\n",
    "            ('num', RobustScaler(), ~dataframe.columns.isin(categorical_columns)),\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_columns),\n",
    "        ]\n",
    "    \n",
    "    preprocessor = ColumnTransformer(transformers=transformers)\n",
    "\n",
    "    # Fit and transform the data\n",
    "    dataframe_transformed = preprocessor.fit_transform(dataframe)\n",
    "\n",
    "    return dataframe_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the ML algorithms part, should we make a function specific for the drift (separated from the ML function)?\n",
    "\n",
    "Should we make two different functions for training and usage or is better to have a singular function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_extraction_pipeline(time_series, kwargs):\n",
    "    \n",
    "    ## Definition of metrics for goodness of model \n",
    "\n",
    "    ## Drift detection algorithms\n",
    "\n",
    "    ### ML ALGORTIHMS (this should be divided on training phase and prediction phase)\n",
    "\n",
    "    ## Check Outliers \n",
    "\n",
    "    ## Feature selection\n",
    "\n",
    "    ## Parameters setting\n",
    "\n",
    "    ## Feature extraction algorithm (maybe more than one, include selection)\n",
    "\n",
    "    ## Models comparison (if needed)\n",
    "        \n",
    "    return extracted_features #as new KPIs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecasting_pipeline(time_series, kwargs):\n",
    "    \n",
    "    ## Definition of metrics for goodness of model \n",
    "\n",
    "    ## Drift detection algorithms\n",
    "\n",
    "    ### ML ALGORTIHMS (this should be divided on training phase and prediction phase)\n",
    "\n",
    "    ## Check Outliers \n",
    "\n",
    "    ## Feature selection\n",
    "\n",
    "    ## Parameters setting\n",
    "\n",
    "    ## Forecasting algorithm (maybe more than one, include selection)\n",
    "\n",
    "    ## Models comparison (if needed)\n",
    "        \n",
    "    return forecasted_feature #forecasting prediction as a new KPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anomalies_detection_pipeline(time_series, kwargs):\n",
    "    \n",
    "    ## Definition of metrics for goodness of model \n",
    "\n",
    "    ## Drift detection algorithms\n",
    "\n",
    "    ### ML ALGORTIHMS (this should be divided on training phase and prediction phase)\n",
    "\n",
    "    ## Check Outliers \n",
    "\n",
    "    ## Feature selection\n",
    "\n",
    "    ## Parameters setting\n",
    "\n",
    "    ## Anomalies detection algorithm (maybe more than one, include selection)\n",
    "\n",
    "    ## Models comparison (if needed)\n",
    "        \n",
    "    return anomalies_detection_feature #detected anomaly as a new KPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Historical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing historical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on historical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incoming data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
