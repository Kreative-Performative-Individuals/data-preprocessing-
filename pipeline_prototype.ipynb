{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt \n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "\n",
    "# from our documents\n",
    "import information as infor\n",
    "import data_cleaning_functions as dcf\n",
    "import feature_engineering_functions as fef\n",
    "\n",
    "# from Scikit Learn library\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import random\n",
    "\n",
    "#seed for random processes\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following pipeline aims to automatize the preprocessing of the stream data from time series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will assume that we already have part of the dataset, that will be used as historical data to fill missing values or study seasonalities and stationarities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will particular show what to do whenever new data arrive to the system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'smart_app_data.pkl'\n",
    "df = pd.read_pickle(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ensure 'time' column is in datetime format\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "# Sort the dataframe by 'time'\n",
    "df = df.sort_values(by='time')\n",
    "\n",
    "# Define the cutoff date for historical and incoming data\n",
    "cutoff_date = pd.Timestamp('2024-10-01')\n",
    "\n",
    "# Split data into historical (before October 2024) and incoming (after October 2024)\n",
    "# Define the conditions correctly using & and | operators and parentheses for grouping\n",
    "historical_data = df[(df['time'].dt.year < 2024) | ((df['time'].dt.year == 2024) & (df['time'].dt.month < 10))]\n",
    "incoming_data = df[(df['time'].dt.year > 2024) | ((df['time'].dt.year == 2024) & (df['time'].dt.month >= 10))]\n",
    "\n",
    "# Check the first few rows of each dataset to confirm\n",
    "print(\"Historical Data (before October 2024):\")\n",
    "print(historical_data.head())\n",
    "\n",
    "print(\"\\nIncoming Data (after October 2024):\")\n",
    "print(incoming_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of rows in historical data: 47936\n",
      "Number of rows in incoming data: 4256\n",
      "First row in historical data: 2024-03-01 00:00:00+00:00\n",
      "Last row in historical data: 2024-09-30 00:00:00+00:00\n",
      "\n",
      "First row in incoming data: 2024-10-01 00:00:00+00:00\n",
      "Last row in incoming data: 2024-10-19 00:00:00+00:00\n"
     ]
    }
   ],
   "source": [
    "# Check the lengths to confirm the split\n",
    "print(f\"\\nNumber of rows in historical data: {len(historical_data)}\")\n",
    "print(f\"Number of rows in incoming data: {len(incoming_data)}\")\n",
    "\n",
    "# Check first and last rows of historical and incoming data\n",
    "print(f\"First row in historical data: {historical_data.iloc[0]['time']}\")\n",
    "print(f\"Last row in historical data: {historical_data.iloc[-1]['time']}\")\n",
    "\n",
    "print(f\"\\nFirst row in incoming data: {incoming_data.iloc[0]['time']}\")\n",
    "print(f\"Last row in incoming data: {incoming_data.iloc[-1]['time']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All machines are: \n",
      "['Large Capacity Cutting Machine 1', 'Assembly Machine 2', 'Laser Cutter', 'Riveting Machine', 'Assembly Machine 3', 'Medium Capacity Cutting Machine 1', 'Medium Capacity Cutting Machine 2', 'Low Capacity Cutting Machine 1', 'Testing Machine 3', 'Medium Capacity Cutting Machine 3', 'Testing Machine 1', 'Testing Machine 2', 'Laser Welding Machine 1', 'Laser Welding Machine 2', 'Large Capacity Cutting Machine 2', 'Assembly Machine 1']\n",
      "All KPIs are: \n",
      "['working_time', 'consumption_idle', 'consumption_working', 'power', 'consumption', 'offline_time', 'idle_time', 'cost', 'cost_working', 'cost_idle', 'cycles', 'good_cycles', 'bad_cycles', 'average_cycle_time']\n"
     ]
    }
   ],
   "source": [
    "# Divide data by machine and kpis (time series)\n",
    "# Retrieve unique machines and KPIs from the DataFrame\n",
    "machines = df['name'].unique().tolist()\n",
    "\n",
    "print(\"All machines are: \")\n",
    "print(machines)\n",
    "\n",
    "kpis = df['kpi'].unique().tolist()\n",
    "\n",
    "print(\"All KPIs are: \")\n",
    "print(kpis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary about specific preprocessing information for kpis\n",
    "# So the kwargs that will be used in the pipelines are stored there\n",
    "# The idea is to implement it somewhere else and importing it\n",
    "\n",
    "# Create the structure for storing batch and counter for each kpi that is produced in the company.\n",
    "info = {}\n",
    "\n",
    "for m in list(info.machine.keys()):\n",
    "    info_asset = {}  # Reset for each machine\n",
    "    for id in list(infor.machine[m]):\n",
    "        info_kpi = {}  # Reset for each asset\n",
    "        for k in list(infor.kpi.keys()):\n",
    "            info_kpi[k] = [[[], [], [], [], []], [0, 0, 0, 0, 0]]  # One space for the batch and the other for the counter\n",
    "        info_asset[id] = info_kpi  # Associate KPIs with the asset ID\n",
    "    info[m] = info_asset  # Associate assets with the machine type\n",
    "    \n",
    "\n",
    "# At the end we have a nested dictionary where the atomic element can be retrieved as:\n",
    "# info['machine_type']['asset_id']['kpi'] --> [batch, counter]\n",
    "# The batch is a list of 5 sub-lists each containing the batch corrisponding to a specific features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if x:\n",
    "    x=x[0]\n",
    "    nan_cons_thr=3\n",
    "    im=infoManager(x)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline will receive as an input the new incoming data for a specific machine and kpi. Also, to perform the preprocessing it need also to receive the batch of a fixed amount of past data and the information about how specifically handle that kpi for that machine (given by kwargs). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_pipeline(batch, new_input, kwargs):\n",
    "\n",
    "    ####### Preprocessing of the data\n",
    "\n",
    "    # Resampling (if needed)\n",
    "    if kwargs.get('resample', False):\n",
    "        new_input= pf.resample_data(batch, new_input, kwargs) #not implemented yet\n",
    "\n",
    "    # Smoothing (if needed, based on kwargs)\n",
    "    if kwargs.get('smooth', False):\n",
    "        new_input = pf.smooth_data(batch, new_input, kwargs) #not implemented yet\n",
    "\n",
    "    ### DATA CLEANING\n",
    "\n",
    "    if x:\n",
    "        x=x[0]\n",
    "        nan_cons_thr=3\n",
    "        im=infoManager(x)\n",
    "\n",
    "        # Try imputation with mean or the HWES model.\n",
    "        for f in features:\n",
    "            batch = im.get_batch(f)\n",
    "            if pd.isna(x[f]):\n",
    "                    counter=im.update_counter(f)\n",
    "                    x[f]=predict_missing(batch)\n",
    "            else: \n",
    "                counter=im.update_counter(f, True)\n",
    "            if counter>nan_cons_thr:\n",
    "                point_id='/'.join(map(str, list(x.values())[1:4]+list([f])))\n",
    "                print(\"It's been \" + str(counter) + ' days that [' + str(point_id) + '] is missing')\n",
    "        print(f'after imputation dp: {x}')\n",
    "\n",
    "        # Check again the consistency of features and the range.\n",
    "        if check_f_consistency(x) and check_range(x)[1]:\n",
    "            pass\n",
    "        else:  # It means that the imputed data point has not passed the check on the features and on their expected range.\n",
    "            # In this case we use the LVCF as a method of imputation since it ensures the respect of these conditiono (the last point in the batch has been preiovusly checked)\n",
    "            for f in features:\n",
    "                batch=im.get_batch(f)\n",
    "                x[f]=batch[-1]\n",
    "\n",
    "        print(f'after check again dp: {x}')\n",
    "        \n",
    "        # In the end update batches with the new data point\n",
    "        for f in features:\n",
    "            print(f'original batch {f}: {im.get_batch(f)}')\n",
    "            im.update_batch(f, x[f])\n",
    "            print(f'batch after update {f}: {im.get_batch(f)}')\n",
    "\n",
    "\n",
    "    ### FEATURE ENGINEERING\n",
    "    new_batch = pd.concat([batch, new_input]).sort_values(by='timestamp') \n",
    "    # Remove the oldest row (the first row after sorting)\n",
    "    new_batch = new_batch.iloc[1:]\n",
    "\n",
    "    return new_batch, new_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering_pipeline(dataframe, kwargs):\n",
    "    ## Check stationarity \n",
    "    # (output is False if not stationary, True if it is, None if test couldn't be applied)\n",
    "    is_stationary = pf.adf_test(dataframe.dropna()) \n",
    "    \n",
    "    ## Check seasonality\n",
    "    # (output: period of the seasonality None if no seasonalaty was detected.\n",
    "    seasonality_period = pf.detect_seasonality_acf(dataframe)\n",
    "\n",
    "    #further check in the case the seasonality pattern is complex and cannot be detected\n",
    "    if seasonality_period == None:\n",
    "        # (output: period of the seasonality None if no seasonalaty was detected.\n",
    "        seasonality_period = pf.detect_seasonality_fft(dataframe)\n",
    "    \n",
    "    # (output: the decomposed time series in a list, of form [trend, seasonal, residual],\n",
    "    # None if it isn't sufficient data or if some error occurs.\n",
    "    decompositions = pf.seasonal_additive_decomposition(dataframe, seasonality_period) \n",
    "\n",
    "    #Make data stationary / Detrend / Deseasonalize (if needed)\n",
    "    \n",
    "    make_stationary = kwargs.get('need_stationarity', False)  # Set default to False if not provided\n",
    "    detrend = kwargs.get('detrend', False) \n",
    "    deseasonalize = kwargs.get('deseasonalize', False) \n",
    "    get_residuals = kwargs.get('get_residuals', False) \n",
    "    \n",
    "    if make_stationary and not is_stationary:\n",
    "        dataframe = pf.make_stationary_decomp(dataframe, decompositions)\n",
    "        is_stationary = pf.adf_test(dataframe.dropna())\n",
    "        if not is_stationary:\n",
    "            dataframe = pf.make_stationary_diff(dataframe)\n",
    "\n",
    "    if detrend:\n",
    "        if decompositions != None:\n",
    "            dataframe = pf.rest_trend(dataframe, decompositions)\n",
    "        else:\n",
    "            dataframe = pf.make_stationary_diff(dataframe)\n",
    "\n",
    "    if deseasonalize:\n",
    "        if decompositions != None:\n",
    "            dataframe = pf.rest_seasonality(dataframe, decompositions)\n",
    "        else:\n",
    "            dataframe = pf.make_stationary_diff(dataframe,seasonality_period=[7]) #default weekly\n",
    "    \n",
    "    if get_residuals:\n",
    "        if decompositions != None:\n",
    "            dataframe = pf.get_residuals(dataframe, decompositions)\n",
    "        else:\n",
    "            dataframe = pf.make_stationary_diff(dataframe)\n",
    "            dataframe = pf.make_stationary_diff(dataframe,seasonality_period=[7]) #default weekly\n",
    "            dataframe = pf.make_stationary_diff(dataframe,seasonality_period=[30]) #default monthly\n",
    "\n",
    "    # Normalize data and apply encoding (if needed)\n",
    "    categorical_columns = dataframe.select_dtypes(include=['object']).columns\n",
    "\n",
    "    # Determine if encoder or scaler should be applied based on kwargs\n",
    "    encoder = kwargs.get('encoder', False)  # Set default to False if not provided\n",
    "    scaler = kwargs.get('scaler', True)  # Set default to True if not provided\n",
    "    \n",
    "    # Apply transformations based on encoder and scaler flags\n",
    "    if not encoder:  # Only scaler\n",
    "        transformers = [('num', RobustScaler(), ~dataframe.columns.isin(categorical_columns))]\n",
    "    elif not scaler:  # Only encoder\n",
    "        transformers = [('cat', OneHotEncoder(handle_unknown='ignore'), categorical_columns)]\n",
    "    else:  # Both encoder and scaler\n",
    "        transformers = [\n",
    "            ('num', RobustScaler(), ~dataframe.columns.isin(categorical_columns)),\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_columns),\n",
    "        ]\n",
    "    \n",
    "    preprocessor = ColumnTransformer(transformers=transformers)\n",
    "\n",
    "    # Fit and transform the data\n",
    "    dataframe_transformed = preprocessor.fit_transform(dataframe)\n",
    "\n",
    "    return dataframe_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the ML algorithms part, should we make a function specific for the drift (separated from the ML function)?\n",
    "\n",
    "Should we make two different functions for training and usage or is better to have a singular function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_extraction_pipeline(time_series, kwargs):\n",
    "    \n",
    "    ## Definition of metrics for goodness of model \n",
    "\n",
    "    ## Drift detection algorithms\n",
    "\n",
    "    ### ML ALGORTIHMS (this should be divided on training phase and prediction phase)\n",
    "\n",
    "    ## Check Outliers \n",
    "\n",
    "    ## Feature selection\n",
    "\n",
    "    ## Parameters setting\n",
    "\n",
    "    ## Feature extraction algorithm (maybe more than one, include selection)\n",
    "\n",
    "    ## Models comparison (if needed)\n",
    "        \n",
    "    return extracted_features #as new KPIs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecasting_pipeline(time_series, kwargs):\n",
    "    \n",
    "    ## Definition of metrics for goodness of model \n",
    "\n",
    "    ## Drift detection algorithms\n",
    "\n",
    "    ### ML ALGORTIHMS (this should be divided on training phase and prediction phase)\n",
    "\n",
    "    ## Check Outliers \n",
    "\n",
    "    ## Feature selection\n",
    "\n",
    "    ## Parameters setting\n",
    "\n",
    "    ## Forecasting algorithm (maybe more than one, include selection)\n",
    "\n",
    "    ## Models comparison (if needed)\n",
    "        \n",
    "    return forecasted_feature #forecasting prediction as a new KPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anomalies_detection_pipeline(time_series, kwargs):\n",
    "    \n",
    "    ## Definition of metrics for goodness of model \n",
    "\n",
    "    ## Drift detection algorithms\n",
    "\n",
    "    ### ML ALGORTIHMS (this should be divided on training phase and prediction phase)\n",
    "\n",
    "    ## Check Outliers \n",
    "\n",
    "    ## Feature selection\n",
    "\n",
    "    ## Parameters setting\n",
    "\n",
    "    ## Anomalies detection algorithm (maybe more than one, include selection)\n",
    "\n",
    "    ## Models comparison (if needed)\n",
    "        \n",
    "    return anomalies_detection_feature #detected anomaly as a new KPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Historical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing historical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on historical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incoming data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
