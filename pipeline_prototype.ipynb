{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt \n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "\n",
    "# from our documents\n",
    "import preprocessing_functions as pf\n",
    "\n",
    "# from Scikit Learn library\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import random\n",
    "\n",
    "#seed for random processes\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following pipeline aims to automatize the preprocessing of the stream data from time series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will assume that we already have part of the dataset, that will be used as historical data to fill missing values or study seasonalities and stationarities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will particular show what to do whenever new data arrive to the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'smart_app_data.pkl'\n",
    "df = pd.read_pickle(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: (213, 8)\n",
      "Test set size: (92, 8)\n"
     ]
    }
   ],
   "source": [
    "# Split the data into historical and future data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide data by machine and kpis (time series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary about specific preprocessing information for kpis\n",
    "# So the kwargs that will be used in the pipelines are stored there\n",
    "# The idea is to implement it somewhere else and importing it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline will receive as an input the new incoming data for a specific machine and kpi. Also, to perform the preprocessing it need also to receive the batch of a fixed amount of past data and the information about how specifically handle that kpi for that machine (given by kwargs). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_pipeline(batch, new_input, kwargs):\n",
    "\n",
    "    ####### Preprocessing of the data\n",
    "\n",
    "    # Resampling (if needed)\n",
    "    if kwargs.get('resample', False):\n",
    "        dataframe = pf.resample_data(dataframe, kwargs) #not implemented yet\n",
    "\n",
    "    # Smoothing (if needed, based on kwargs)\n",
    "    if kwargs.get('smooth', False):\n",
    "        dataframe = pf.smooth_data(dataframe, kwargs) #not implemented yet\n",
    "\n",
    "    ### DATA CLEANING\n",
    "\n",
    "    ## Data type standardization\n",
    "\n",
    "    ## Check for inconsistencies\n",
    "    \n",
    "    ## Fill missing values\n",
    "\n",
    "    ### FEATURE ENGINEERING\n",
    "    dataframe = pd.concat([batch, new_input]).sort_values(by='timestamp') \n",
    "\n",
    "    ## Check stationarity\n",
    "    is_stationary = pf.adf_test(dataframe.dropna()) #False if not stationary, True if it is, None if test couldn't be applied\n",
    "    \n",
    "    ## Check seasonality \n",
    "    period_of_observation = kwargs.get('period_of_observation', None)\n",
    "    if period_of_observation is None:\n",
    "        raise ValueError(\"Period of observation must be provided in kwargs.\")\n",
    "    \n",
    "    trend, seasonalilty, residual = pf.seasonal_additive_decomposition(dataframe, period_of_observation) \n",
    "\n",
    "    #Make data stationary / Detrend / Deseasonalize (if needed)\n",
    "    \n",
    "    make_stationary = kwargs.get('need_stationarity', False)  # Set default to False if not provided\n",
    "    detrend = kwargs.get('detrend', False) \n",
    "    deseasonalize = kwargs.get('deseasonalize', False) \n",
    "    \n",
    "    if make_stationary and not is_stationary:\n",
    "        dataframe = pf.make_stationary(dataframe, kwargs)\n",
    "\n",
    "    if detrend and deseasonalize:\n",
    "        dataframe = pf.rest_trend(dataframe, kwargs)\n",
    "        dataframe = pf.rest_seasonality(dataframe, kwargs)\n",
    "    elif detrend:\n",
    "        dataframe = pf.rest_trend(dataframe, kwargs)\n",
    "    elif deseasonalize:\n",
    "        dataframe = pf.rest_seasonality(dataframe, kwargs)\n",
    "\n",
    "    # Normalize data and apply encoding (if needed)\n",
    "    categorical_columns = dataframe.select_dtypes(include=['object']).columns\n",
    "\n",
    "    # Determine if encoder or scaler should be applied based on kwargs\n",
    "    encoder = kwargs.get('encoder', False)  # Set default to False if not provided\n",
    "    scaler = kwargs.get('scaler', True)  # Set default to True if not provided\n",
    "    \n",
    "    # Apply transformations based on encoder and scaler flags\n",
    "    if not encoder:  # Only scaler\n",
    "        transformers = [('num', RobustScaler(), ~dataframe.columns.isin(categorical_columns))]\n",
    "    elif not scaler:  # Only encoder\n",
    "        transformers = [('cat', OneHotEncoder(handle_unknown='ignore'), categorical_columns)]\n",
    "    else:  # Both encoder and scaler\n",
    "        transformers = [\n",
    "            ('num', RobustScaler(), ~dataframe.columns.isin(categorical_columns)),\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_columns),\n",
    "        ]\n",
    "    \n",
    "    preprocessor = ColumnTransformer(transformers=transformers)\n",
    "\n",
    "    # Fit and transform the data\n",
    "    dataframe_transformed = preprocessor.fit_transform(dataframe)\n",
    "\n",
    "    return dataframe_transformed\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ML_pipeline(time_series, kwargs):\n",
    "    \n",
    "    ## Definition of metrics for goodness of model \n",
    "\n",
    "    ## Drift detection algorithms\n",
    "\n",
    "    ### ML ALGORTIHMS (this should be divided on training phase and prediction phase)\n",
    "\n",
    "    ## Check Outliers \n",
    "\n",
    "    ## Feature selection\n",
    "\n",
    "    ## Parameters setting\n",
    "\n",
    "    ## Data forecasting (if selected)\n",
    "\n",
    "    ## Anomalies detection (if selected)\n",
    "\n",
    "    ## Models comparison (if needed)\n",
    "        \n",
    "    return result_of_ML #forecasting prediction or detected anomaly\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
